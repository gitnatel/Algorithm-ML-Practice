{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small command to ensure this code runs correctly on all notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **1 | Search Algorithm Implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.1 | Problem Description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 | Initial State.\n",
    "\n",
    "To initialise the problem, the elements that make up the initial state are defined.\n",
    "\n",
    "This includes the layout of the bays, all positional indicators, the held container, and a dictionary containing the weights of the boxes.\n",
    "\n",
    "For this implementation, all containers other than 2 and 4 are treated as light, so specifically defining them is unnecessary.\n",
    "\n",
    "These elements are then put together into a list, named \"initial_state\".\n",
    "\n",
    "The possible actions are also defined here, in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bays = [[],[],[],[2,3,4],[1,5],[],[],[6]]\n",
    "initial_agent_pos = 0\n",
    "initial_held_container = None\n",
    "initial_cost = 0\n",
    "initial_target_bay = 0\n",
    "\n",
    "weight = {2: 'heavy', 4: 'heavy'}\n",
    "\n",
    "initial_state = [initial_bays, initial_agent_pos, initial_held_container, initial_target_bay, weight]\n",
    "\n",
    "all_actions = [\"LEFT\", \"RIGHT\", \"PICK\", \"DROP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 | Goal State.\n",
    "\n",
    "To finish building the problem, a goal state is defined. The function \"is_goal_state(state)\" can be called to determine if a given state meets the conditions required. These being; The bay at the agent's position being empty, and the target bay being the location of containers 1,2 and 3, sequentially.\n",
    "\n",
    "The goal state function can be tested by calling \"is_goal_state(initial_state)\", which gives \"False\", as the initial state does not meet the criteria. Rearranging the initial state to satisfy the goal will give \"True\".\n",
    "\n",
    "The final addition to the goal state defines that the held container is none. While this is not strictly in the goal state requirements, it helps avoid an issue where the agent picks up a box, thereby satisfying the goal condition and returning the path, when moving over the obstruction is almost always slightly more efficient in terms of cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_goal_state(state):\n",
    "    if state[0][state[1]] == [] and state[0][state[3]] == [1, 2, 3] and state[2] == None:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "is_goal_state(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 1.2 | Basic Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 | Print State.\n",
    "\n",
    "Next, the basic functionality allowing a state to be printed is defined in the function \"print_state(state)\". This is unnecessary for the solution, but allows both any given state to be printed, and for the solution to be animated later.\n",
    "\n",
    "This function can be tested by calling \"print_state(initial_state)\", which gives a graphical representation of the initial state, with all relevant information printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state(state):\n",
    "    for i in range(len(state[0])):\n",
    "        if i == state[1]:\n",
    "            print(str(state[0][i]) + \" <-[\" + str(state[2]) + \"]-\")\n",
    "        else:\n",
    "            print(str(state[0][i]))\n",
    "    print(\"\\nTarget bay: \" + str(state[3]))\n",
    "    print()\n",
    "\n",
    "print_state(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 | Is Action Valid.\n",
    "\n",
    "The function allowing action validity to be determined is defined as \"is_action_valid(state, action, prev_action)\".\n",
    "\n",
    "These arguments allow for a significantly less complicated search tree compared to omitting this function.\n",
    "\n",
    "Any function that is the direct opposite of the previously made action is invalid, as that would be redundant.\n",
    "\n",
    "Also outlined here is logic disallowing \"PICK\" and \"DROP\" when the held container is \"not None\" and \"None\" respectively.\n",
    "\n",
    "This also disallows \"DROP\" when the current bay is full, and \"PICK\" when the current bay is empty.\n",
    "\n",
    "\"LEFT\" and \"RIGHT\" cannot be taken if the agent is at the left boundary and right boundary respectively.\n",
    "\n",
    "The underscores are used as junk variables, to deconstruct state properly. They are not useful to this specific function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_action_valid(state, action, prev_action):\n",
    "    \n",
    "    bays, agent_pos, held_container, _, _ = state\n",
    "\n",
    "    if prev_action == \"LEFT\" and action == \"RIGHT\" or \\\n",
    "       prev_action == \"RIGHT\" and action == \"LEFT\" or \\\n",
    "       prev_action == \"PICK\" and action == \"DROP\" or \\\n",
    "       prev_action == \"DROP\" and action == \"PICK\":\n",
    "        return False\n",
    "\n",
    "    if action == \"PICK\" and not bays[agent_pos]:\n",
    "        return False\n",
    "\n",
    "    if action == \"PICK\" and held_container is not None:\n",
    "        return False\n",
    "\n",
    "    if action == \"DROP\" and len(bays[agent_pos]) >= 3:\n",
    "        return False\n",
    "\n",
    "    if action == \"DROP\" and held_container is None:\n",
    "        return False\n",
    "\n",
    "    if action == \"LEFT\" and agent_pos == 0:\n",
    "        return False\n",
    "\n",
    "    if action == \"RIGHT\" and agent_pos == 7:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 | Apply Action.\n",
    "\n",
    "The function to apply a given action to a given state is defined as \"apply_action(state, action, prev_action=None)\".\n",
    "\n",
    "For this function to work, copy is imported, as it is needed to create a working duplicate of the state.\n",
    "\n",
    "First, the action validity is checked. This helps minimize the search tree complexity. If the action is not valid, the original state is returned.\n",
    "\n",
    "This will be ignored by the searching algorithm, as repeat states are skipped.\n",
    "\n",
    "Each action is then defined along with its impact on the state, and the associated cost.\n",
    "\n",
    "The listed cost is the minutes required to perform the action.\n",
    "\n",
    "It takes into account the weight of the held container for \"LEFT\" and \"RIGHT\", and the state of the current bay for \"PICK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def apply_action(state, action, prev_action=None):\n",
    "    \n",
    "    if not is_action_valid(state, action, prev_action):\n",
    "        return state, 0\n",
    "\n",
    "    new_state = copy.deepcopy(state)\n",
    "    cost = 0\n",
    "\n",
    "    if action == \"LEFT\":\n",
    "        new_state[1] -= 1\n",
    "        if new_state[4].get(new_state[2]) == \"heavy\":\n",
    "            cost = 5\n",
    "        else:\n",
    "            cost = 3\n",
    "\n",
    "    if action == \"RIGHT\":\n",
    "        new_state[1] += 1\n",
    "        if new_state[4].get(new_state[2]) == \"heavy\":\n",
    "            cost = 5\n",
    "        else:\n",
    "            cost = 3\n",
    "\n",
    "    if action == \"PICK\":\n",
    "        cost = 4 - len(new_state[0][new_state[1]])\n",
    "        new_state[2] = new_state[0][new_state[1]].pop()\n",
    "\n",
    "    if action == \"DROP\":\n",
    "        new_state[0][new_state[1]].append(new_state[2])\n",
    "        new_state[2] = None\n",
    "        cost = 2\n",
    "\n",
    "    return new_state, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 | Apply Actions.\n",
    "\n",
    "The function to apply a series of actions to an initial state is defined as \"apply_actions(state,actions)\".\n",
    "\n",
    "This function calculates the total cost of the series of actions, as well as returning the state that is reached after applying them all sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_actions(state,actions):\n",
    "\n",
    "    new_state = state\n",
    "    total_cost = 0\n",
    "\n",
    "    for action in actions:\n",
    "        new_state, cost = apply_action(new_state, action)\n",
    "        total_cost = total_cost + cost\n",
    "        \n",
    "    return new_state, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 1.3 | Searching Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 | Heuristic.\n",
    "\n",
    "This function calculates an estimate of the cost between a given state and the goal state, and is defined as \"heuristic(state)\".\n",
    "\n",
    "It works by initially unpacking the state, ignoring held container as cost estimations are done based on the plan of what to pick up rather than the current held container.\n",
    "\n",
    "It then estimates the cost of removing any obstacles from the target bay, by adding the cost to pick an obstacle, the cost to move it, and the cost to drop it\n",
    "\n",
    "It then estimates the cost of getting to the next necessary container, moving it to the target bay, and placing it correctly.\n",
    "\n",
    "All of this is returned as the total heuristic, providing a good estimate of the cost between a given state and the goal. This heuristic should not overestimate the cost and therefore be both optimal. If the tree is finite, it is also complete.\n",
    "\n",
    "There is room for improvement, however this is a great trade off between function and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(state):\n",
    "\n",
    "    heuristic = 0\n",
    "    \n",
    "    bays, agent_pos, _, target_bay, weights = state\n",
    "\n",
    "    for i in range(len(bays[target_bay])):\n",
    "        if bays[target_bay][i] != i + 1:\n",
    "            heuristic += 4 - len(bays[target_bay])\n",
    "            heuristic += abs(agent_pos - target_bay) * (5 if weights.get(bays[target_bay][i]) == \"heavy\" else 3)\n",
    "            heuristic += 2\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        if i not in bays[target_bay]:\n",
    "            for bay_pos, bay in enumerate(bays):\n",
    "                if i in bay:\n",
    "                    heuristic += abs(agent_pos - bay_pos) * (5 if weights.get(i) == \"heavy\" else 3)\n",
    "                    heuristic += 4 - len(bay)\n",
    "                    heuristic += abs(bay_pos - target_bay) * (5 if weights.get(i) == \"heavy\" else 3)\n",
    "                    heuristic += 2\n",
    "                    break\n",
    "\n",
    "    return heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 | A* Search.\n",
    "\n",
    "This A* search function works by importing heapq, which is a more efficient way of managing priority than manually managing the frontier.\n",
    "\n",
    "It then initialises the frontier, and explored states.\n",
    "\n",
    "The search takes the highest priority state, checks if it is the goal, adds it to explored, and then moves on to continue the search if these conditions aren't met.\n",
    "\n",
    "The search tries each action by testing its validitiy, then applying it. This is returned as a cost, which is added to the heuristic of that state.\n",
    "\n",
    "This is then returned as the new priority for the state, and added to the frontier.\n",
    "\n",
    "It will loop through this until a solution is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def a_star_search(initial_state):\n",
    "    \n",
    "    frontier = [(heuristic(initial_state), initial_state, [], 0)]\n",
    "    heapq.heapify(frontier)\n",
    "    explored = set()\n",
    "\n",
    "    while frontier:\n",
    "        _, current_state, path, path_cost = heapq.heappop(frontier)\n",
    "\n",
    "        if str(current_state) in explored:\n",
    "            continue\n",
    "\n",
    "        if is_goal_state(current_state):\n",
    "            return path\n",
    "\n",
    "        explored.add(str(current_state))\n",
    "\n",
    "        for action in all_actions:\n",
    "            if not is_action_valid(current_state, action, path[-1] if path else None):\n",
    "                continue\n",
    "\n",
    "            new_state, cost = apply_action(current_state, action)\n",
    "            new_path = path + [action]\n",
    "            new_path_cost = path_cost + cost\n",
    "            priority = new_path_cost + heuristic(new_state)\n",
    "            heapq.heappush(frontier, (priority, new_state, new_path, new_path_cost))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 1.4 | Algorithm Results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 | Path Calculation.\n",
    "\n",
    "This is where we call the search on the initial state. It will return the solution found, along with the total cost of the path.\n",
    "\n",
    "It works generally in under 5 seconds on trees of a complexity of less than 50 actions, which is incredibly rare to exceed even when the puzzle is fully randomised.\n",
    "\n",
    "The initial state provided should return a path of 37 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = a_star_search(initial_state)\n",
    "\n",
    "if path is None:\n",
    "    print(\"No solution found.\")\n",
    "else:\n",
    "    final_state, total_cost = apply_actions(initial_state, path)\n",
    "    print_state(final_state)\n",
    "    print(\"Path to goal:\", path)\n",
    "    print(\"\\nNumber of actions:\", len(path))\n",
    "    print(\"\\nTotal cost:\", total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 | Path Animation.\n",
    "\n",
    "A small addition to animate the path using the print_state function, and clearing the output.\n",
    "\n",
    "This path makes what appears to be a mistake, by not moving 3 up near the goal bay before moving 2, however the cost is identical. This was tested by grabbing the path output, and manually changing it to what seemed to make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def animate_path(initial_state, path):\n",
    "        \n",
    "        current_state = copy.deepcopy(initial_state)\n",
    "        \n",
    "        for action in path:\n",
    "            current_state, _ = apply_action(current_state, action)\n",
    "            clear_output(wait=True)\n",
    "            print_state(current_state)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "animate_path(initial_state, path)\n",
    "\n",
    "final_state, total_cost = apply_actions(initial_state, path)\n",
    "\n",
    "print(\"Path to goal:\", path)\n",
    "print(\"\\nNumber of actions:\", len(path))\n",
    "print(\"\\nTotal cost:\", total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 1.5 | Algorithm Performance on a Random Input.\n",
    "\n",
    "**The entirety of section 1.5 can be re-run to test the algorithm on a new random input.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 | Containers Randomiser.\n",
    "\n",
    "Considering the performance of the search on a relatively complex input, it can be tested on a completely random input.\n",
    "\n",
    "This function allows the containers to be distributed randomly throughout a blank initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def distribute_containers(bays):\n",
    "\n",
    "    bays = copy.deepcopy(bays)\n",
    "    containers = list(range(1, 7))\n",
    "    \n",
    "    random.shuffle(containers)\n",
    "\n",
    "    for container in containers:\n",
    "        random.shuffle(bays)\n",
    "\n",
    "        for bay in bays:\n",
    "            if len(bay) < 3:\n",
    "                bay.append(container)\n",
    "                break\n",
    "    \n",
    "    return bays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 | Random Initial State.\n",
    "\n",
    "This function builds the random intial state, by generating a random value for all variables, except held container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_bays = [[],[],[],[],[],[],[],[]]\n",
    "random_target_bay = random.randint(0, 7)\n",
    "random_agent_pos = random.randint(0, 7)\n",
    "random_bays = distribute_containers(blank_bays)\n",
    "random_state = [random_bays, random_agent_pos, initial_held_container, random_target_bay, weight]\n",
    "\n",
    "print_state(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 | Random Path Calculation.\n",
    "\n",
    "This calls the search on the random initial state, and prints the solution.\n",
    "\n",
    "On average, this is significantly less complex than the first initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_path = a_star_search(random_state)\n",
    "if path is None:\n",
    "    print(\"No solution found.\")\n",
    "else:\n",
    "    random_final_state, random_total_cost = apply_actions(random_state, random_state_path)\n",
    "\n",
    "    print_state(random_final_state)\n",
    "    print(\"Path to goal:\", random_state_path)\n",
    "    print(\"\\nNumber of actions:\", len(random_state_path))\n",
    "    print(\"\\nTotal cost:\", random_total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 | Random Path Animation.\n",
    "\n",
    "This animates the path found for the random initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_path(random_state, random_state_path)\n",
    "\n",
    "random_final_state, random_total_cost = apply_actions(random_state, random_state_path)\n",
    "\n",
    "print(\"Path to goal:\", random_state_path)\n",
    "print(\"\\nNumber of actions:\", len(random_state_path))\n",
    "print(\"\\nTotal cost:\", random_total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **2 | Machine Learning Implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.1 | Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 | Reading File.\n",
    "\n",
    "This section reads the csv file containing the data to be trained and tested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rawdata = pd.read_csv('ContainerData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 | Target & Feature Selection.\n",
    "\n",
    "Here the target and features are selected, informing the model what to predict for, and what to query the model with when testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"Priority\"]\n",
    "features = [\"Height\",\"Width\",\"Hue\",\"Times moved\",\"Hours\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 | Spltting Testing and Training Data.\n",
    "\n",
    "Here the training and test data are split, with a test size of 0.25, meaning 25% of the data will be omitted from training to use as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(rawdata, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.2 | Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 | Training Variables.\n",
    "\n",
    "Here we rename the split data into four variables, for readability.\n",
    "\n",
    "train_target is flattened into a one dimesnional array to avoid an error when applying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_features = train[features]\n",
    "train_target = train[target].to_numpy()\n",
    "\n",
    "test_features = test[features]\n",
    "test_target = test[target].to_numpy()\n",
    "\n",
    "train_target = np.ravel(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 | Selecting Machine Learning Model.\n",
    "\n",
    "Classification is selected as the model, as it is most suited for predicting where the solution is one of a few specific options. 'High' and 'Low' priority classification are a perfect use case for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Classification = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 | Training the Model.\n",
    "\n",
    "Here the model is given the training data, and adjusts appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification.fit(train_features,train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.3 | Machine Learning Results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 | Generating Predicitons.\n",
    "\n",
    "The testing data, with the exception of the target, is given to the model trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_test_set = Classification.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 | Evaluating Accuracy.\n",
    "\n",
    "This final section outputs the accuracy when comparing the expected classification with the predicted classification.\n",
    "\n",
    "The output should be around 0.9, which is a 90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_target, predictions_on_test_set)\n",
    "\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
